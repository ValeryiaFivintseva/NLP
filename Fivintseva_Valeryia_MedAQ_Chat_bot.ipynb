{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ld8omkaWR1J"
      },
      "source": [
        "# Создание чат-бота\n",
        "\n",
        "Чатбот на 2 интента:\n",
        "- Простая болталка\n",
        "- Вопрос-ответ в качестве доврачебной медицинской консультации\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6oSHCbjsXJqH"
      },
      "outputs": [],
      "source": [
        "! pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1IEmLBwvXnx4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification, AutoModelForCausalLM\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJFPDOpyY7X1",
        "outputId": "e7b567fa-1d82-4063-f87e-e94a346d8c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "path =  \"/content/drive/My Drive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmYkhAHjZ__4"
      },
      "source": [
        "Загрузим токенайзер и модель классификатора интентов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzmdB1ChZJ2L",
        "outputId": "41ad1cee-80c6-4f96-8cf5-c5fd87ee03d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/My Drive/model_intent_classifier/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer_intent_classifier = AutoTokenizer.from_pretrained(path + 'tokenizer_intent_classifier/')\n",
        "model_intent_classifier = TFAutoModelForSequenceClassification.from_pretrained(path + 'model_intent_classifier/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfCtCL5wajVZ"
      },
      "source": [
        "Загрузим токенайзер и модель GPT для \"болталки\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_oPq3J1la9Ue"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATjc9ZNcaCFc",
        "outputId": "b55f9417-5a0a-4f93-df11-fc5ed3c08445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer_dialog = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "model_dialog = AutoModelForCausalLM.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wng1AHT3bHzZ"
      },
      "source": [
        "Загрузим токенайзер и модель GPT для вопросно-ответной системы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "05VypK9bbDb8"
      },
      "outputs": [],
      "source": [
        "tokenizer_qa_medical_gen = AutoTokenizer.from_pretrained(path + 'tokenizer_qa_medical_gen')\n",
        "model_qa_medical_gen = AutoModelForCausalLM.from_pretrained(path + 'model_qa_medical_gen').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZMbW9P83neF"
      },
      "source": [
        "Напишем класс для диалоговой системы DialogSystem.\n",
        "\n",
        "**Основные методы объекта класса DialogSystem:**\n",
        "- intent_classifier - классифицирует входящее сообщение на 3 интента. Если текущий интент меняется, обнуляет историю сообщений.\n",
        "- dialog_generator - генерирует ответы для двух интентов - болталки и медицинской вопросно-ответной системы. При разных интентах на вход подаются разные GPT-модели\n",
        "- search_article - поиск 3-х наиболее подходящих отзывов в ответ на сообщение. Для 3-го интента\n",
        "- Объединяет все эти методы метод get_response. Именно он и будет вызываться как внешняя функция.\n",
        "- Также в классе есть вспомогательные функции для выполнения принципа DRY (Don't Repeat Yourself)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s3M3CLhUdwRB"
      },
      "outputs": [],
      "source": [
        "class DialogSystem:\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 tokenizer_intent_classifier,\n",
        "                 model_intent_classifier,\n",
        "                 tokenizer_dialog,\n",
        "                 model_dialog,\n",
        "                 tokenizer_qa_medical_gen,\n",
        "                 model_qa_medical_gen,\n",
        "                 lenght_history = 10):\n",
        "        self.context = [] # Список, хранящий lenght_history предыдущих сообщений\n",
        "        self.current_intent = 0 # Текущий интент. При изменении интента обнуляется self.context\n",
        "        self.tokenizer_intent_classifier = tokenizer_intent_classifier\n",
        "        self.model_intent_classifier = model_intent_classifier\n",
        "        self.tokenizer_dialog = tokenizer_dialog\n",
        "        self.model_dialog = model_dialog\n",
        "        self.tokenizer_qa_medical_gen = tokenizer_qa_medical_gen\n",
        "        self.model_qa_medical_gen = model_qa_medical_gen\n",
        "        self.lenght_history = lenght_history\n",
        "\n",
        "\n",
        "    def intent_classifier(self, sentence):\n",
        "        sent_token = self.tokenizer_intent_classifier(sentence, padding=\"max_length\", truncation=True, return_tensors='tf')\n",
        "        predict_intent = self.model_intent_classifier(sent_token).logits.numpy().argmax()\n",
        "        if predict_intent != self.current_intent:\n",
        "            self.context = []\n",
        "        self.current_intent = predict_intent\n",
        "        print(f'current intent: {predict_intent}')\n",
        "        return predict_intent\n",
        "\n",
        "\n",
        "    def custom_append(self, a, b):\n",
        "        a.append(b)\n",
        "        if len(a) > self.lenght_history:\n",
        "            a.pop(0)\n",
        "\n",
        "\n",
        "    def respond_to_dialog(self, model, tokenizer, texts):\n",
        "        prefix = '\\nx:'\n",
        "        for i, t in enumerate(texts):\n",
        "            prefix += t\n",
        "            prefix += '\\nx:' if i % 2 == 1 else '\\ny:'\n",
        "        tokens = tokenizer(prefix, return_tensors='pt')\n",
        "        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
        "        end_token_id = tokenizer.encode('\\n')[0]\n",
        "        size = tokens['input_ids'].shape[1]\n",
        "        output = model.generate(\n",
        "            **tokens,\n",
        "            eos_token_id=end_token_id,\n",
        "            do_sample=True,\n",
        "            max_length=size+128,\n",
        "            repetition_penalty=3.2,\n",
        "            temperature=1,\n",
        "            num_beams=3,\n",
        "            length_penalty=0.01,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        decoded = tokenizer.decode(output[0])\n",
        "        result = re.findall(r'\\ny:(.+)', decoded)[-1]\n",
        "        return result.strip()\n",
        "\n",
        "\n",
        "    def dialog_generator(self, model, tokenizer, sentence):\n",
        "        self.custom_append(self.context, sentence)\n",
        "        result = self.respond_to_dialog(model, tokenizer, self.context)\n",
        "        self.custom_append(self.context, result)\n",
        "        return result\n",
        "\n",
        "    def get_response(self, sentence):\n",
        "        predict_intent = self.intent_classifier(sentence)\n",
        "        if predict_intent == 0:\n",
        "            res = self.dialog_generator(self.model_dialog, self.tokenizer_dialog, sentence)\n",
        "        else:\n",
        "            res = self.dialog_generator(self.model_qa_medical_gen, self.tokenizer_qa_medical_gen, sentence)\n",
        "        return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4bJ1-EV6AXS"
      },
      "source": [
        "Создадим объект responser класса DialogSystem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UDzPtZKlxu26"
      },
      "outputs": [],
      "source": [
        "responser = DialogSystem(tokenizer_intent_classifier,\n",
        "                         model_intent_classifier,\n",
        "                         tokenizer_dialog,\n",
        "                         model_dialog,\n",
        "                         tokenizer_qa_medical_gen,\n",
        "                         model_qa_medical_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtcUFD1a6P8Y"
      },
      "source": [
        "Создадим тестовую функцию, аналогичную callback-функции в MessageHandler, чтобы протестировать работу нашей диалоговой системы, не подключаясь к api telegram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q5HlPZ7Aw2_E"
      },
      "outputs": [],
      "source": [
        "def test_text_message():\n",
        "    message = input()\n",
        "    answer = responser.get_response(message)\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KZ9Kuv266KJ"
      },
      "source": [
        "Создадим бесконечный цикл с вызовом тестовой функции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "TMtu7wyyzPn8",
        "outputId": "affe6ebf-8cfd-43c3-fd1c-f51adff47b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Привет. Как дела?\n",
            "current intent: 0\n",
            "Все отлично.\n",
            "Погода хорошая, как думаешь?\n",
            "current intent: 1\n",
            "Здравствуйте. У меня такой же вопрос; Здравствуйте! Мне 28 лет, у меня есть проблемы с почками и суставами, в основном это остеохондроз позвоночника, грыжа межреберного сустава поясничного сплетения, протрузии на коленных суставах, артроз нижних тазобедренных суставов, кистозная болезнь, атеросклероз сосудов шейного отдела позвоночника, ревматоидный артрит грудного отдела позвоночника, хронический панкреатит, нарушение работы поджелудочной железы, гастрит, холецистит, дисбактериоз желчного пузыря, гепатит\n",
            "Что посоветуешь принимать, если болит голова второй день?\n",
            "current intent: 1\n",
            "Анализ мочи, общий анализ крови, УЗИ органов брюшной полости, мочевого пузыря, почек, мочеиспускания, биохимия крови, кровь на гормоны щитовидной железы, хеликобактер пилори, эндокринные заболевания, амилазу, эритроциты, иммуноглобулин Е, лейкоциты, креатинин, тестостерон, кальций, магний, железо, медь, гематокрит, фенибут, фосфолипиды, билирубин, калий, цинк, фолиевую кислоту, витамины группы\n",
            "Не слишком много лишнего?\n",
            "current intent: 0\n",
            "Да, очень мало.\n",
            "Хорошо, но ещё болит и горло - это что значит?\n",
            "current intent: 1\n",
            "Здравствуйте! Это может быть из-за. Проверьте уровень гормонов щитовидной железы.; Здравствуйте, проверьте уровень гормонов щитовидной железы, сдайте кровь на гормоны щитовидной железы; Выполните узи щитовидной железы,проверьте поджелудочную железу, выполните УЗИ органов брюшной полости, покажитесь гастроэнтерологу для исключения воспалительных процессов в желчном пузыре, исключите патологий пищевода, поджелудочной железы, эндокринную патологию шейного отдела позвоночника, грыжи межреберных мышц, остеохондроз грудного отдела позвоночника,\n",
            "К какому врачу идти, если насморк и горло болит?\n",
            "current intent: 1\n",
            "Выполните исследование уровня гормонов щитовидной железы,сдайте анализ крови на гормоны щитовидной железы,определившись с уровнем гормонов,выполните УЗИ органов брюшной полости,покажитесь гастроэнтерологу для исключения воспалительных процессов в желчном пузыре, исключите патологий пищевода, эндокринную патологию шейного отдела позвоночника, грыжи межреберных мышц, остеохондроз грудного отдела позвоночника, остеохондроз грудного отдела позвоночника, грыжи межреберных мышц, остеохондроз грудного отдела позвоночника, остеохондроз грудного отдела позвоночника,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-12e94566dee1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_text_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-ee968d787f1e>\u001b[0m in \u001b[0;36mtest_text_message\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_text_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    test_text_message()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmfi40jw8ZGb"
      },
      "source": [
        "## Создание telegram-bot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcy5s1Vh7f8V",
        "outputId": "f636c748-c7bc-4c7d-fd14-a5ed285ea9be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot==13.13 in /usr/local/lib/python3.10/dist-packages (13.13)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.13) (2024.2.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.13) (6.3.2)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.13) (3.6.3)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.13) (2023.4)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.13) (4.2.2)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.13) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.13) (1.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.13) (5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-telegram-bot==13.13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PJzspmFjBeRO"
      },
      "outputs": [],
      "source": [
        "from telegram import Update"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext"
      ],
      "metadata": {
        "id": "Al6e5xdGQ99l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elg5J6cSFwwa",
        "outputId": "d8195eda-85f2-4d62-dfcf-45a1394f253f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-7a8079a55ed9>:17: TelegramDeprecationWarning: The argument `clean` of `start_polling` is deprecated. Please use `drop_pending_updates` instead.\n",
            "  updater.start_polling(clean=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current intent: 0\n",
            "current intent: 0\n",
            "current intent: 1\n",
            "current intent: 1\n",
            "current intent: 0\n",
            "current intent: 0\n"
          ]
        }
      ],
      "source": [
        "updater = Updater('6735041502:AAFXosAjDAMiuMPrK279xTLaaXentvsDm-w') # Токен API к Telegram\n",
        "dispatcher = updater.dispatcher\n",
        "\n",
        "def startCommand(update: Update, context: CallbackContext):\n",
        "    text = 'Вас приветствует бот MedAQ!\\nПообщайтесь или задайте вопрос по беспокоящим Вас симптомам.'\n",
        "    update.message.reply_text(text)\n",
        "\n",
        "def textMessage(update: Update, context: CallbackContext):\n",
        "    input_txt = update.message.text\n",
        "    answer = responser.get_response(input_txt)\n",
        "    update.message.reply_text(answer)\n",
        "\n",
        "start_command_handler = CommandHandler('start', startCommand)\n",
        "text_message_handler = MessageHandler(Filters.text, textMessage)\n",
        "dispatcher.add_handler(start_command_handler)\n",
        "dispatcher.add_handler(text_message_handler)\n",
        "updater.start_polling(clean=True)\n",
        "updater.idle()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}